# -*- coding: utf-8 -*-
"""Copy of multiple_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1optbVmtVZ4vEHXLCoDllra4IYNpjtBWY

# Multiple Linear Regression

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:,:-1].values
y = dataset.iloc[:,-1].values

print(X)

"""## Encoding categorical data"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
# Object of column transformer class
ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[3])],remainder='passthrough')
# transformers=[('kind of transformation',what kind of encoding,[indices])],remainder=Columns that will not be encoded)
X = np.array(ct.fit_transform(X))# the Matrix we need to transform ie. X, ct.fit... doesnt return x as nparray, therefore we u np.array

print(X)

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =0)

# No need of Feature Scaling as the coefficient will take care of it

"""## Training the Multiple Linear Regression model on the Training set"""

# We do not need to eliminate a dummy variable as the sklearn class will automatically
# take care of it, same for backward elimination (Feature selection)
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

"""## Predicting the Test set results"""

y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2) # Display Numerical value till 2 decimals places after point
# Here we compare ypred with y_test as we cannot display many feature in a plot (5 dimension)
print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)), axis = 1))
# np.concatenate(y_pred.reshape(no of rows,no of column),y_pred.reshape(len(y_pred),1)), ) 
# axis = 0=> vertical concatenation, 1 => horizontal concatination
# output y_pred is horizontal vector,to convert from horizontal to vertical we use reshape.

print(regressor.predict([[1, 0, 0, 160000, 130000, 300000]]))
# the values of the features were all input in a double pair of square brackets[[]]. 
# That's because the "predict" method always expects a 2D array as the format of its inputs.

print(regressor.coef_)
print(regressor.intercept_)
# Profit=86.6×Dummy State 1−873×Dummy State 2+786×Dummy State 3+0.773×R&D Spend+0.0329×Administration+0.0366×Marketing Spend+42467.53
