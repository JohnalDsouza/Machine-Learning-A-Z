# -*- coding: utf-8 -*-
"""Copy of data_preprocessing_tools.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uI5dbgJhRj2g0-neOSEOxQ-1bD7nk1tg

# Data Preprocessing Tools

## Importing the libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset"""

dataset = pd.read_csv('Data.csv')# Output of a function by pandas (Read values of dataset and create a data frame)
X = dataset.iloc[:,:-1].values # FEATURES iloc locate indices and extracts the column we wish to use [rowsrange, column range]
y = dataset.iloc[:,-1].values

print(X)

print(y)

"""## Taking care of missing data"""

from sklearn.impute import SimpleImputer
# object/ instance-> imputer, class -> SimpleImputer 
imputer = SimpleImputer(missing_values=np.nan, strategy ='mean') 
imputer.fit(X[:,1:3]) #look at missing value and compute average of the salary
X[:,1:3] = imputer.transform(X[:,1:3]) # to replace the missing data

print(X[:,1:3])

"""## Encoding categorical data

### Encoding the Independent Variable
"""

# One-Hot-Encoding replating string data with vectors, because if we use numbers the
# machine learning algorithm will think there is some co-relationship
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
# Object of column transformer class
ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')
# transformers=[('kind of transformation',what kind of encoding,[indices])],remainder=Columns that will not be encoded)
X = np.array(ct.fit_transform(X))# the Matrix we need to transform ie. X, ct.fit... doesnt return x as nparray, therefore we u np.array

print(X)

"""### Encoding the Dependent Variable"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder() # le object of class LabelEncoder
y = le.fit_transform(y) # No need to convert to np.array

print(y)

# Feature scaling should be performed only after splitting the dataset into the Training set and Test set,
# to prevent data from the test set being leaked into the train set

"""## Splitting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =1)

print(X_train)

print(X_test)

print(y_train)

print(y_test)

"""## Feature Scaling"""

# Standardization works all the time,
# while normalization works only when the dataset in normally distributed
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
# Do not use Feature scaling for vectors of the dummy column
# fit ->compute the mean and standard deviation, transform -> use stardization fomula
X_train[:,3:] = sc.fit_transform(X_train[:,3:]) 
# use only transform on test set, because we need to use the same scalar we used on the training set,
# cannot get a new scalar
X_test[:,3:] = sc.transform(X_test[:,3:])

print(X_train)

print(X_test)